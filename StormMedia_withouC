from urllib.request import urlopen
from bs4 import BeautifulSoup
import time
import random
import json


#選取政治版面頁數
storm_news = []    
p=1
for page in range(1,5):
    href = 'https://www.storm.mg/category/118/'+str(page)
    print(page)
    response = urlopen(href)
    html = BeautifulSoup(response)
    #找到特定範圍在選取網址列
    newf = html.find("div",class_="middle_category_cards")
    #選取所有單篇網址列
    news = newf.find_all("div",class_="category_card")
    #迴圈抓取整頁新聞連結
    for r in news:
        newsd={}
        #title=r.find("h3",class_="card_title").text
        link=r.find("a",class_="card_link")["href"]
        response = urlopen(link)
        html= BeautifulSoup(response)
        ti=html.find("h1", id="article_title").text
        
        da=html.find("span", class_="info_time").text
        au=html.find("span", class_="info_author").text.replace("新新聞","")
        te=html.find("div", id="CMS_wrapper").text.replace("\n","").replace("\t","")
        kw=html.find("div", id="tags_list_wrapepr").text.split()
        img=html.find("img", id="feature_img")["src"]
        #print("=======================")
        # print("source","風傳媒")
        #print("title",ti)
        #print("url",link)
        #print("img",img)
       # print("date_",da)
       # print("type","net")
       # print("kw",kw)
       # print("author",au)
       # print("content",te)
       
        newsd={"source":"NOWnews","title":ti,"url":link,"date_":da,"type":"net","author":au,"kw":kw,"content":te,"img_url":img}
        storm_news.append(newsd)
      
        time.sleep(random.randint(0,3))
    time.sleep(random.randint(0,3))

    if (page%50)==0:
        filename= "StormMedia_20180919_20181109_" + str(p)
        with open(filename, 'w', encoding="utf-8") as outfile:
            json.dump(storm_news, outfile)
        storm_news = [] 
        p=p+1
